ViPE リポジトリ概要

  ViPE (Video Pose Engine) は、NVIDIAがオープンソースで公開している空間AI
  ツールです。生の動画から以下の3D幾何情報を高精度に抽出します：

  - カメラ内部パラメータ（光学特性）
  - カメラの動き・姿勢（軌跡）
  - 密で準メトリックな深度マップ

  ---
  フォルダ構成

  vipe/
  ├── vipe/                    # メインPythonパッケージ
  │   ├── cli/                 # CLIインターフェース (infer, visualize)
  │   ├── pipeline/            # 処理パイプライン
  │   ├── slam/                # DROID-SLAMベースのVisual SLAM
  │   ├── priors/              # 推定モデル群
  │   │   ├── depth/           # 深度推定（6種以上のモデル対応）
  │   │   ├── geocalib/        # カメラキャリブレーション
  │   │   └── track_anything/  # 物体追跡（SAM + DINO）
  │   ├── streams/             # 動画/フレームのI/O
  │   ├── ext/                 # CUDA拡張
  │   └── utils/               # ユーティリティ
  ├── csrc/                    # C++/CUDA拡張コード
  ├── configs/                 # Hydra設定ファイル
  ├── scripts/                 # ユーティリティスクリプト
  ├── envs/                    # 環境ファイル
  └── run.py                   # パイプライン実行スクリプト

  ---
  主要コンポーネント

  | モジュール                 | 説明
       |
  |-----------------------|----------------------------------------------------|
  | vipe/slam/            |
  DROID-SLAMベースのSLAMシステム（カメラ姿勢・シーン構造推定）              |
  | vipe/priors/depth/    | Metric3D, UniDepth,
  VideoDepthAnythingなど6種以上の深度モデル |
  | vipe/priors/geocalib/ | GeoCalibによるカメラ内部パラメータ推定
               |
  | vipe/pipeline/        | 処理パイプラインの統合（DefaultAnnotationPipeline）
             |

  ---
  対応カメラモデル

  - Pinhole（通常カメラ）
  - Wide-angle / Fisheye（広角・魚眼）
  - 360° Panorama（全天球）

  ---
  技術スタック

  - PyTorch 2.7 + CUDA 12.8
  - Hydra による設定管理
  - カスタムCUDAカーネル（lietorch, DROID-NET等）
  - Viser / Rerun による3D可視化

  ---
  データセット

  HuggingFaceで公開されている大規模データセット:
  - Dynpose-100K++: 99,501動画 / 1580万フレーム
  - Wild-SDG-1M: 96.6万動画 / 7820万フレーム
  - Web360: 2,114本のパノラマ動画


  これで必要な情報が揃いました。以下に設定と実行方法をまとめます。

  ---
  車載広角カメラでの推論設定

  1. 推論の実行コマンド

  CLIを使用する場合（最も簡単）:
  vipe infer YOUR_VIDEO.mp4 --pipeline wide_angle --output vipe_results

  run.pyを使用する場合（より細かい制御）:
  python run.py \
    pipeline=wide_angle \
    streams=raw_mp4_stream \
    streams.base_path=YOUR_VIDEO.mp4

  ---
  2. wide_angle パイプラインの設定内容

  configs/pipeline/wide_angle.yaml:
  defaults:
    - default

  init:
    camera_type: "mei"  # MEI = Modified Equidistant Intercouncil (広角/魚眼用)

  slam:
    keyframe_depth: unidepth-l

  post:
    depth_align_model: null  # 広角では深度アラインメントなし

  output:
    viz_attributes: [['rgb', 'instance'], ['rectified', 'pcd']]

  ポイント:
  - camera_type: "mei" → 広角・魚眼レンズの歪みを考慮したカメラモデル
  - intrinsics: "geocalib" → GeoCalibでカメラ内部パラメータを自動推定

  ---
  3. カメラ軌跡の出力形式

  推論後、以下のファイルが生成されます：

  | パス                                       | 内容                           |
  |------------------------------------------|------------------------------|
  | vipe_results/pose/{video_name}.npz       | カメラ姿勢（4x4 camera-to-world行列） |
  | vipe_results/intrinsics/{video_name}.npz | カメラ内部パラメータ [fx, fy, cx, cy]  |

  ---
  4. 軌跡データの読み込み方法

  import numpy as np

  # poseファイルを読み込み
  data = np.load("vipe_results/pose/your_video.npz")

  # フレームインデックス
  frame_indices = data["inds"]  # (N,)

  # カメラ姿勢行列 (camera-to-world)
  poses = data["data"]  # (N, 4, 4)

  # 車載カメラの軌跡（カメラ位置）を抽出
  trajectory = poses[:, :3, 3]  # (N, 3) - X, Y, Z座標

  print(f"フレーム数: {len(frame_indices)}")
  print(f"軌跡の形状: {trajectory.shape}")

  ---
  5. 3D可視化

  vipe visualize vipe_results/
  ブラウザで3D点群とカメラ軌跡を確認できます。

  ---
  6. COLMAPフォーマットへの変換（オプション）

  他のツールで使いたい場合：
  python scripts/vipe_to_colmap.py vipe_results/ --sequence your_video

  ---
  補足: カスタム設定

  車載カメラ特有の設定が必要な場合、独自のYAMLを作成できます：

  # configs/pipeline/dashcam.yaml
  defaults:
    - wide_angle

  init:
    instance:
      phrases:  # 動的オブジェクトの検出対象
        - person
        - vehicle
        - car
        - bus
        - truck
        - motorcycle
        - bicycle

  実行時：
  python run.py pipeline=dashcam streams.base_path=YOUR_VIDEO.mp4



  調査結果をまとめます。

  ---
  内部パラメータ既知の場合の設定方法

  現在のViPEには、既知のintrinsicsを直接コマンドラインから入力する公式な方法はありません。ただし、以下の方法で対応可能です。

  ---
  方法1: カスタムProcessorを作成（推奨）

  vipe/pipeline/processors.py に以下のようなProcessorを追加し、パイプラインで使用します：

  class KnownIntrinsicsProcessor(StreamProcessor):
      """既知のカメラ内部パラメータを設定するProcessor"""

      def __init__(
          self,
          fx: float,
          fy: float,
          cx: float,
          cy: float,
          camera_type: CameraType = CameraType.MEI,
          distortion: list[float] | None = None,  # MEIの場合は [xi] を指定
      ) -> None:
          super().__init__()
          self.fx = fx
          self.fy = fy
          self.cx = cx
          self.cy = cy
          self.camera_type = camera_type
          self.distortion = distortion or []

      def update_attributes(self, previous_attributes: set[FrameAttribute]) -> set[FrameAttribute]:
          return previous_attributes | {FrameAttribute.INTRINSICS, FrameAttribute.CAMERA_TYPE}

      def __call__(self, frame_idx: int, frame: VideoFrame) -> VideoFrame:
          frame.intrinsics = torch.as_tensor(
              [self.fx, self.fy, self.cx, self.cy] + self.distortion
          ).float()
          frame.camera_type = self.camera_type
          return frame

  ---
  方法2: Configでoptimize_intrinsics: falseを設定

  configs/pipeline/wide_angle.yaml を以下のように修正：

  defaults:
    - default

  init:
    camera_type: "mei"
    intrinsics: "gt"  # これでoptimize_intrinsicsがfalseになる

  slam:
    keyframe_depth: unidepth-l
    optimize_intrinsics: false  # 明示的に指定

  post:
    depth_align_model: null

  ただし、この場合も初期値をどこかで与える必要があります。

  ---
  方法3: Pythonスクリプトで直接実行

  run.pyを参考に、カスタムスクリプトを作成：

  import torch
  from pathlib import Path
  from vipe.streams.raw_mp4_stream import RawMP4Stream
  from vipe.streams.base import ProcessedVideoStream, AssignAttributesProcessor, FrameAttribute
  from vipe.pipeline.default import DefaultAnnotationPipeline
  from vipe.utils.cameras import CameraType
  from omegaconf import OmegaConf

  # 既知のカメラパラメータ（広角カメラの例）
  KNOWN_INTRINSICS = {
      'fx': 500.0,      # 焦点距離 x
      'fy': 500.0,      # 焦点距離 y
      'cx': 640.0,      # 光学中心 x（画像幅/2）
      'cy': 360.0,      # 光学中心 y（画像高さ/2）
      'xi': 1.0,        # MEIモデルの歪みパラメータ
  }

  class KnownIntrinsicsProcessor:
      def __init__(self, intrinsics: dict, camera_type: CameraType):
          self.intrinsics = torch.tensor([
              intrinsics['fx'],
              intrinsics['fy'],
              intrinsics['cx'],
              intrinsics['cy'],
              intrinsics.get('xi', 0.0),  # MEIの場合
          ]).float()
          self.camera_type = camera_type

      def update_attributes(self, prev):
          return prev | {FrameAttribute.INTRINSICS, FrameAttribute.CAMERA_TYPE}

      def __call__(self, idx, frame):
          frame.intrinsics = self.intrinsics.clone()
          frame.camera_type = self.camera_type
          return frame

  # 使用例
  video_path = "your_dashcam_video.mp4"
  stream = RawMP4Stream(Path(video_path), seek_range=range(0, 1000, 1))

  # 既知のintrinsicsを適用
  processed_stream = ProcessedVideoStream(
      stream,
      [KnownIntrinsicsProcessor(KNOWN_INTRINSICS, CameraType.MEI)]
  )

  # パイプライン設定
  config = OmegaConf.create({
      'init': {'camera_type': 'mei', 'instance': None},
      'slam': {
          'keyframe_depth': 'unidepth-l',
          'optimize_intrinsics': False,  # 既知なので最適化しない
      },
      'post': {'depth_align_model': None},
      'output': {
          'path': 'vipe_results/',
          'save_viz': True,
          'save_artifacts': True,
          # ...
      }
  })

  # 実行
  pipeline = DefaultAnnotationPipeline(**config)
  result = pipeline.run(processed_stream)

  ---
  MEI（広角）カメラモデルのパラメータ

  広角カメラの場合、CameraType.MEIを使用します。intrinsicsの形式は：

  [fx, fy, cx, cy, xi]

  - fx, fy: 焦点距離（ピクセル単位）
  - cx, cy: 光学中心（通常は画像中心）
  - xi: MEI歪みパラメータ（広角の度合い。0=pinhole, >0=広角）

  ---
  注意点

  1. intrinsicsを固定する場合は slam.optimize_intrinsics: false を必ず設定
  2. 広角カメラの場合、xiパラメータの設定が精度に大きく影響
  3. カメラのキャリブレーションツール（OpenCVなど）で事前に取得した値を使用

  既知パラメータがあれば、GeoCalibによる推定をスキップできるため、処理速度の向上と精度の安定化が期待できます。


  重要な違いがあります。確認した内容をまとめます。

  ---
  OpenCV歪みモデル vs MEIモデル

  結論: 直接変換できません。全く異なるカメラモデルです。

  ---
  1. OpenCV Radial-Tangential モデル (k1, k2, p1, p2, k3)

  2D画像平面上で歪みを補正：
  r² = x² + y²
  x_distorted = x(1 + k1·r² + k2·r⁴ + k3·r⁶) + 2·p1·x·y + p2(r² + 2x²)
  y_distorted = y(1 + k1·r² + k2·r⁴ + k3·r⁶) + p1(r² + 2y²) + 2·p2·x·y

  2. MEI (Unified Omnidirectional) モデル (ξ / xi)

  3D→2D投影時に球面投影を適用：
  r = √(X² + Y² + Z²)
  u = fx · X / (Z + ξ·r) + cx
  v = fy · Y / (Z + ξ·r) + cy

  | パラメータ     | 意味            | 範囲           |
  |-----------|---------------|--------------|
  | ξ = 0     | Pinhole（歪みなし） | -            |
  | ξ = 1     | 半球投影          | -            |
  | 0 < ξ < 1 | 一般的な広角カメラ     | 典型的: 0.5〜0.9 |

  ---
  対処方法

  方法A: GeoCalibに推定させる（推奨）

  OpenCVパラメータがあっても、MEIのξはGeoCalibで自動推定させるのが最も簡単です：

  vipe infer your_video.mp4 --pipeline wide_angle

  GeoCalibは画像から直接MEIパラメータを推定するので、変換の必要がありません。

  ---
  方法B: OCamCalibで再キャリブレーション

  MEIモデル用のキャリブレーションツールを使用：

  - OCamCalib (MATLAB): https://sites.google.com/site/scaraborubino/ocamcalib-omnidirectional-camera-calibration-toolbox-for-matlab
  - Kalibr (ROS): MEIモデルをサポート

  ---
  方法C: 近似変換（精度は低い）

  OpenCVのk1から大まかにξを推定する経験則：

  import numpy as np

  def approximate_xi_from_opencv(k1, k2, image_width, image_height, fx):
      """
      OpenCV歪みパラメータからMEI xiを近似推定
      注意: これは近似値であり、正確ではありません
      """
      # 画像端での歪み量から推定
      r_max = np.sqrt((image_width/2)**2 + (image_height/2)**2) / fx

      # OpenCVモデルでの画像端の歪み係数
      distortion_factor = 1 + k1 * r_max**2 + k2 * r_max**4

      # 魚眼の度合いに応じたξの近似
      if distortion_factor > 1:
          # Barrel distortion（樽型歪み）→ pinholeに近い
          xi_approx = 0.0
      else:
          # Pincushion distortion（糸巻き型歪み）or 広角
          # 経験的な近似式
          xi_approx = min(1.0, abs(1 - distortion_factor) * 2)

      return xi_approx

  # 例
  k1 = -0.3  # OpenCVのk1（負の値は樽型歪み）
  k2 = 0.1
  image_width = 1920
  image_height = 1080
  fx = 800

  xi_estimate = approximate_xi_from_opencv(k1, k2, image_width, image_height, fx)
  print(f"推定 xi: {xi_estimate}")

  ---
  方法D: fx, fy, cx, cyのみ使用（ξはGeoCalibで推定）

  既知の内部パラメータとGeoCalibを組み合わせる：

  class HybridIntrinsicsProcessor(StreamProcessor):
      """fx, fy, cx, cyは既知値を使い、xiのみGeoCalibで推定"""

      def __init__(self, video_stream, fx, fy, cx, cy):
          super().__init__()
          self.fx, self.fy, self.cx, self.cy = fx, fy, cx, cy

          # GeoCalibでξのみ推定
          model = GeoCalib(weights="distorted").cuda()
          first_frame = next(iter(video_stream)).rgb.moveaxis(-1, 0)[None]
          res = model.calibrate(first_frame, camera_model="simple_mei")

          # 推定されたξを取得
          self.xi = res["camera"].dist[0, 0].item()
          print(f"Estimated xi: {self.xi}")

      def __call__(self, frame_idx, frame):
          frame.intrinsics = torch.tensor([
              self.fx, self.fy, self.cx, self.cy, self.xi
          ]).float()
          frame.camera_type = CameraType.MEI
          return frame

  ---
  推奨アプローチ

  車載広角カメラの場合：

  1. fx, fy, cx, cyが正確に分かっている → 方法Dで使用
  2. 全パラメータをGeoCalibに任せる → --pipeline wide_angleでそのまま実行
  3. 高精度が必要 → OCamCalibでMEIパラメータを直接キャリブレーション

  OpenCVの歪みパラメータ(k1, k2, p1, p2, k3)は、ViPEのMEIモデルとは互換性がないため、再キャリブレーションまたはGeoCalibによる推定が必要です。


  調査結果をまとめます。

  ---
  ViPEがメートルスケールの姿勢推定を実現する仕組み

  1. 単眼SLAMの「スケール曖昧性」問題

  通常の単眼Visual SLAMでは、絶対スケールが不定（scale ambiguity）という根本的な問題があります：

  カメラ軌跡とシーンを同時に2倍に拡大しても、
  画像上では全く同じに見える
  → メートル単位のスケールを決定できない

  ---
  2. ViPEの解決策: メトリック深度モデルの統合

  ViPEは学習ベースのメトリック深度推定モデルをSLAMに統合することで、この問題を解決しています。

  ┌─────────────────────────────────────────────────────────────┐
  │                    ViPE パイプライン                         │
  │                                                              │
  │  ┌──────────────┐     ┌──────────────┐     ┌─────────────┐ │
  │  │   入力画像    │────▶│ UniDepth等   │────▶│メトリック深度│ │
  │  │              │     │(学習済みモデル)│     │  (メートル)   │ │
  │  └──────────────┘     └──────────────┘     └──────┬──────┘ │
  │         │                                          │        │
  │         ▼                                          ▼        │
  │  ┌──────────────┐                         ┌──────────────┐ │
  │  │  DROID-SLAM  │◀── 深度制約 (disps_sens)│ BA最適化     │ │
  │  │  (相対姿勢)   │                         │ |d - d_sens|² │ │
  │  └──────────────┘                         └──────────────┘ │
  │         │                                          │        │
  │         └──────────────────┬───────────────────────┘        │
  │                            ▼                                │
  │                   ┌──────────────┐                          │
  │                   │メートルスケール│                          │
  │                   │  カメラ軌跡   │                          │
  │                   └──────────────┘                          │
  └─────────────────────────────────────────────────────────────┘

  ---
  3. 技術的な実装詳細

  Step 1: メトリック深度推定

  vipe/slam/components/buffer.py:233-266 より:

  def update_disps_sens(self, depth_model, frame_idx):
      # 各キーフレームでメトリック深度を推定
      depth_input = DepthEstimationInput(
          rgb=self.images[frame_idx],
          intrinsics=self.intrinsics[0],  # カメラ内部パラメータを使用
          camera_type=self.camera_type,
      )
      # メトリック深度（メートル単位）を取得
      metric_depth = depth_model.estimate(depth_input).metric_depth
      # 逆深度（disparity）に変換してバッファに保存
      self.disps_sens[frame_idx] = metric_depth.reciprocal()

  Step 2: Bundle Adjustmentへの深度制約

  vipe/slam/ba/terms.py:246-268 より:

  class DispSensRegularizationTerm(SolverTerm):
      """
      深度の正則化項:
      E(dense_disp_i) = dense_disp_i - dense_disps_sens_i
      
      SLAMで推定した深度を、メトリック深度モデルの
      出力に近づける制約を追加
      """

  この制約により、SLAMの最適化において深度（とスケール）がメートル単位で固定されます。

  ---
  4. 使用されている深度モデル

  | モデル      | 特徴                     | 深度タイプ              |
  |----------|------------------------|--------------------|
  | UniDepth | カメラ内部パラメータを考慮したメトリック深度 | MODEL_METRIC_DEPTH |
  | Metric3D | 焦点距離からスケールを推定          | METRIC_DEPTH       |

  これらのモデルは大規模データセットで学習されており、単一画像からメートル単位の深度を推定できます。

  ---
  5. なぜ機能するのか

  深度モデルの学習

  UniDepth等のモデルは以下のデータで学習されています：
  - LiDAR付き自動運転データ（KITTI, nuScenes等）
  - RGB-Dセンサーデータ（NYUv2等）
  - 合成データ（正確な深度GT付き）

  これにより、モデルは画像の特徴からメートル単位の深度を推論する能力を獲得しています。

  カメラ内部パラメータの役割

  # UniDepthはカメラの焦点距離を入力として使用
  focal_length = src.intrinsics[0].item()  # fx
  camera = Pinhole(K=K)
  predictions = self.model.infer(rgb, camera)

  焦点距離が既知の場合、画像上のピクセルサイズと実際の3D距離の関係が決まるため、よりが正確なメトリック深度推定が可能になります。

  ---
  6. 精度の限界

  | 要因            | 影響                 |
  |---------------|--------------------|
  | 深度モデルの誤差      | スケールに直接影響          |
  | カメラ内部パラメータの精度 | スケールに影響            |
  | シーンの種類        | 学習データと異なるシーンでは精度低下 |
  | 動的物体          | 深度推定の外れ値を引き起こす     |

  結論: ViPEの「メトリックスケール」は**近似的（near-metric）**であり、絶対的な精度は深度モデルの性能とカメラキャリブレーションの精度に依存します。

  ---
  まとめ

  ViPEのメートルスケール推定 =
    学習済み深度モデル（UniDepth等）
    + カメラ内部パラメータ
    + SLAM最適化への深度制約統合

  これにより、単眼カメラからでも実用的なメートルスケールの3D復元と姿勢推定が可能になっています。